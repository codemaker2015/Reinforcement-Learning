{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "In this notebook, we will implement REINFORCE algorithm to play Hill Climbing.\n",
    "\n",
    "## Step 1: Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepanshu\\.conda\\envs\\ai\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Viewing our Enviroment\n",
    "Execute the code cell below to play Cartpole with a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of state is:  (2,)\n",
      "No. of Actions:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAZCElEQVR4nO3df7BcZ33f8fcHWcgE27EdXXuEJCLVEW1kJsjxHZXGaetighUnVKZTMiI/UGacEdPaKbTuDwumWOpUU5LB0F9jJiL2RBMDivjhWmVIQFFhCDOOxZWRsWRZ+AYr1kW30rXBsZ1QBcnf/rHP2kdXe+/dX2f3PGc/r5mdPfvsObvPI+393O999jm7igjMzCwfrxl2B8zMrDMObjOzzDi4zcwy4+A2M8uMg9vMLDMObjOzzDi4rbIkfVDS7/d73zYeKyT9VD8ey6wM8jpuGwRJvwncCVwDvAA8CGyNiOeH2a9WJAWwJiImW9z3NeCtwNlC8y9ExMMD6p6ZK24rn6Q7gd8B/h3w4zSC7yeBfZJeO8cxFw2uhx27IyIuKVwuCO2K998y5+C2Ukm6DNgO/HZE/ElE/CgijgO/QiO8fz3tt03S5yQ9IOkF4DdT2wOFx3qvpL+U9Jyk/yjpuKS3F45/IG2vStMdmyU9I+lZSR8qPM56SQ9Lel7StKT/OdcvkA7HGpJul/QU8FRq+2+STkh6QdJBSf+wsP82SZ9NY35R0uOS3iRpq6TT6bh3FPb/cUn3pT5/T9J/lrSo135bfhzcVrafAy4GvlBsjIiXgD8GfqHQvBH4HHA58Kni/pLWAvcCvwYso1G5L1/guX8e+LvATcCHJf10aj8H/GtgKfAP0v3/ssNxzeVW4O8Da9PtbwLrgCuBTwOflXRxYf93An8IXAF8C/gyjZ/L5cB/An6vsO8uGlM0PwVcB7wD+K0+9dsy4uC2si0Fno2Isy3um073Nz0cEf8rIl6OiB/O2vefA/87Ir4REX8LfBhY6A2a7RHxw4h4DHgMeAtARByMiD+PiLOp+v894B93MKb/nqr15yU9Ouu+/xIR32/2PyIeiIjn0nPdAyyh8cuk6c8i4svp3+ezwBjwkYj4EbAbWCXpcklXA78IfCAi/joiTgMfBzZ10G+rCc/DWdmeBZZKuqhFeC9L9zedmOdx3lC8PyL+RtJzCzz3/y1s/w1wCYCkNwEfA8aBH6Pxc3Bwgccq+lcRMdcKlvPGkOb3fyv1P4DLOP+X1anC9g9p/JI7V7hN6vcbgMXAtKTm/q+Z/Xw2GlxxW9keBs4A/6zYKOn1NCrI/YXm+SroaWBF4fjXAT/RZZ8+ATxJY+XIZcAHAc1/SNteGUOaz/4PNObzr4iIy4G/6vK5TtD4d1waEZeny2URcW0/Om15cXBbqSLir2i8Ofk/JG2QtFjSKhrTAlM05nfb8TngnZJ+Lr2RuJ3uw/ZSGksSX5L094B/0eXjtPM8Z4EZ4CJJH6ZRcXcsIqaBrwD3SLpM0mskXSOpkykeqwkHt5UuIn6XRlX7URqB+QiNCvKmiDjT5mMcAX6bxrzvNPAicJpGFdqpfwv8anqMTwJ/1MVjtOPLNN6A/Q7wl8D/o7epjfcCrwWeAH5A45fZsh77aBnyCTiWJUmXAM/TmO54etj9MRskV9yWDUnvlPRjaX78o8DjwPHh9sps8EoL7jSfeUzSpKS7ynoeGykbgZPpsgbYFP6T0UZQKVMl6Wyu79A4uWKKxkkI74mIJ/r+ZGZmI6asins9MBkR300nS+ymUS2ZmVmPyjoBZznnv3s+ReM04JaWLl0aq1atKqkrZmb5OX78OM8++2zLJa9lBXerJztvTkbSFmALwBvf+EYmJiZK6oqZWX7Gx8fnvK+sqZIpYGXh9goabyi9IiJ2RsR4RIyPjY2V1A0zs/opK7i/CayRtDqd5bYJ2FvSc5mZjZRSpkoi4qykO2icObYIuD+d+WZmZj0q7dMBI+JLwJfKenwzs1HlMyfNzDLj4DYzy4yD28wsMw5uM7M+ksTBg/36Xo7W/NVlZmYlmCu8r7++98+HcnCbmQ1Qq0DvNMw9VWJmlhlX3GZmA+SpEjOziupHQM/FUyVmZn1WZmiDg9vMLDsObjOzzDi4zcwy4+A2M8uMg9vMLDMObjOzzDi4zcwy4+A2M8uMg9vMLDM9nfIu6TjwInAOOBsR45KuBP4IWAUcB34lIn7QWzfNzKypHxX3P4mIdRExnm7fBeyPiDXA/nTbzMz6pIypko3ArrS9C7i1hOcwMxtZvQZ3AF+RdFDSltR2dURMA6Trq1odKGmLpAlJEzMzMz12w8xsdPT6sa43RMRJSVcB+yQ92e6BEbET2AkwPj5e7kdpmZnVSE8Vd0ScTNengQeB9cApScsA0vXpXjtpZmav6jq4Jb1e0qXNbeAdwGFgL7A57bYZeKjXTpqZ2at6mSq5GnhQUvNxPh0RfyLpm8AeSbcBzwDv7r2bZmbW1HVwR8R3gbe0aH8OuKmXTpmZ2dx85qSZWWb8ZcFmZn2Spo5fuV5IRHcL6hzcZmY9aDek2zm23SB3cJuZdaCXoO7XYzu4zczmsVCYdjvd0c1zNTm4zcxamCtE+xnU8z32+Pj4nPs5uM3MklZhXWZQd8vBbWYjL5fAbnJwm9lI63ZlxzA5uM1sJOUY2E0ObjMbKTkHdpOD28xGQh0Cu8nBbWa1VwztnAO7ycFtZrVVt8Bu8qcDmlktlXlq+rC54jaz2qlrpd3k4DazWmmGdh0Du8nBbWa1UPcqu2jBOW5J90s6Lelwoe1KSfskPZWuryjct1XSpKRjkm4uq+NmZk2jFNrQ3puTfwBsmNV2F7A/ItYA+9NtJK0FNgHXpmPulbSob701MyuQdN7UyCiENrQR3BHxdeD7s5o3ArvS9i7g1kL77og4ExFPA5PA+j711czsFaNWZRd1uxzw6oiYBkjXV6X25cCJwn5Tqe0CkrZImpA0MTMz02U3zGzUjVpoQ//XcbdaONnyXzUidkbEeESMj42N9bkbZlZno7ByZD7dBvcpScsA0vXp1D4FrCzstwI42X33zMzON+qhDd0H915gc9reDDxUaN8kaYmk1cAa4EBvXTQzu/CNyFG24DpuSZ8BbgSWSpoC7gY+AuyRdBvwDPBugIg4ImkP8ARwFrg9Is6V1HczGxGj/EZkKwsGd0S8Z467bppj/x3Ajl46ZWbW5Cr7Qv6QKTOrPIf2+XzKu5lVkivtubniNrPKcWjPz8FtZpXi0F6Yg9vMKsOh3R4Ht5lVgkO7fQ5uMxs6h3ZnHNxmZplxcJvZULna7pyD28yGxqHdHZ+AY2YD588e6Y0rbjMbKId27xzcZjYUDu3uObjNbGA8p90fDm4zGwiHdv84uM2sdA7t/nJwm1mpHNr95+A2s9IUV5BY/ywY3JLul3Ra0uFC2zZJ35N0KF1uKdy3VdKkpGOSbi6r42aWD1fb/dVOxf0HwIYW7R+PiHXp8iUASWuBTcC16Zh7JS3qV2fNLB+eIinPgsEdEV8Hvt/m420EdkfEmYh4GpgE1vfQPzPLkEO7XL2c8n6HpPcCE8CdEfEDYDnw54V9plLbBSRtAbYUbvs/2awGHNrl6/bNyU8A1wDrgGngntTe6p2Ilv97EbEzIsYjYvz6669vHOw3Msyy5tAejK6COyJORcS5iHgZ+CSvTodMASsLu64ATvbWRTMzK+oquCUtK9x8F9BccbIX2CRpiaTVwBrgQDuP2fwN7arbLE+utgdnwTluSZ8BbgSWSpoC7gZulLSOxjTIceB9ABFxRNIe4AngLHB7RJxrtzMRgSTPd5tlxqE9WAsGd0S8p0XzffPsvwPY0UunzCwf/it58Cp35mRxysQvCLNqK1barrYHp3LBDf5zyywHnh4ZnkoGN/jNSjOzuVQ2uMHhbVZVrraHq9LBbWZmF6p8cLvqNquO4qIBV9vDU/ngBoe3WRX429mrI4vgBoe3WVU4tIcvm+AGh7fZsHh6pFqyCm4zM8swuF11mw2Wq+3qyS64weFtNigO7WrKMrjB4W1WNod2dWUb3GZWHhdE1ZZ1cLvqNus/r9euvqyDGxzeZmVxaFdX9sFd5PA2643ntfNQi+Auvsgc3mbdcWjnY8HglrRS0lclHZV0RNL7U/uVkvZJeipdX1E4ZqukSUnHJN1c5gCa/GIzs1HRTsV9FrgzIn4aeCtwu6S1wF3A/ohYA+xPt0n3bQKuBTYA90paVEbnZ/N8t1l3XG3nZcHgjojpiHg0bb8IHAWWAxuBXWm3XcCtaXsjsDsizkTE08AksL7fHZ+nv4DD26xdDu38dDTHLWkVcB3wCHB1RExDI9yBq9Juy4EThcOmUtvsx9oiaULSxMzMTOc9N7OeucDJU9vBLekS4PPAByLihfl2bdF2wa/yiNgZEeMRMT42NtZuN9riqtusM66289JWcEtaTCO0PxURX0jNpyQtS/cvA06n9ilgZeHwFcDJ/nS3fQ5vs/l5iiRf7awqEXAfcDQiPla4ay+wOW1vBh4qtG+StETSamANcKB/Xe6cw9vsfA7tvF3Uxj43AL8BPC7pUGr7IPARYI+k24BngHcDRMQRSXuAJ2isSLk9Is71vedtiIhXXqCS/CI1w6FdBwsGd0R8g9bz1gA3zXHMDmBHD/3qm2J4m5nVQS3OnFyI57vNGlxt18NIBDc4vM0c2vUxMsFtNspcsNTLSAW3q24bRf587foZqeAGh7eNLod2fYxccIPD20aH57XraSSD28wsZyMb3K66re5cbdfXyAY3OLytvhza9TbSwQ0Ob6sfh3b9jXxwm9WJC5DR4ODGVbfVg9drjw4Ht5lZZhzcSbHqduVtuSnOa7varj8Hd4Ff8GaWAwf3LJ7vttx4FcnocXC34PC2XDi0R5ODew4Ob6s6h/boaufLgldK+qqko5KOSHp/at8m6XuSDqXLLYVjtkqalHRM0s1lDsBsFLmgGG3tfFnwWeDOiHhU0qXAQUn70n0fj4iPFneWtBbYBFwLvAH4U0lvGtYXBvei+X2V/qJhqyq/LkfTghV3RExHxKNp+0XgKLB8nkM2Arsj4kxEPA1MAuv70dlh8JSJVY2nSKyjOW5Jq4DrgEdS0x2Svi3pfklXpLblwInCYVPMH/TZcHjbsDm0DToIbkmXAJ8HPhARLwCfAK4B1gHTwD3NXVscfsGrTNIWSROSJmZmZjru+CAVf0gc3jYsDm1raiu4JS2mEdqfiogvAETEqYg4FxEvA5/k1emQKWBl4fAVwMnZjxkROyNiPCLGx8bGehnDQPiHxcyqop1VJQLuA45GxMcK7csKu70LOJy29wKbJC2RtBpYAxzoX5eHx/PdNiyutq2onVUlNwC/ATwu6VBq+yDwHknraEyDHAfeBxARRyTtAZ6gsSLl9hxXlMzFK01s0BzaNtuCwR0R36D1vPWX5jlmB7Cjh36ZGf7rzlrzmZNd8JSJDYI/X9vm4uDuksPbBsWhbbM5uHvg8LayeF7b5uPg7hOHt/WLQ9sW4uDukX+4zGzQHNx94CkT6xdX29YOB3efOLytVw5ta5eDu48c3tYth7Z1wsHdZw5v65RD2zrl4DYzy4yDuwSuuq1drratGw7ukji8bSEObeuWg3sAHN42m0PbeuHgLlFEuPK2Czi0rVcO7gFweFuTQ9v6wcFtNiD+xW394uAeEFfd1uRq23rl4B4gh/fo8hSJ9ZODe8Ac3qPHoW391s63vF8s6YCkxyQdkbQ9tV8paZ+kp9L1FYVjtkqalHRM0s1lDiBnDu/6c2hbGdqpuM8Ab4uItwDrgA2S3grcBeyPiDXA/nQbSWuBTcC1wAbgXkmLyuh8rrxMcDQ4tK0sCwZ3NLyUbi5OlwA2ArtS+y7g1rS9EdgdEWci4mlgEljf117XhMO7vhzaVqa25rglLZJ0CDgN7IuIR4CrI2IaIF1flXZfDpwoHD6V2mY/5hZJE5ImZmZmehmDWaX4F7GVra3gjohzEbEOWAGsl/TmeXZv9aq9oOyIiJ0RMR4R42NjY+31toZcdddLsdJ2tW1l6WhVSUQ8D3yNxtz1KUnLANL16bTbFLCycNgK4GTPPa0xh7eZdaKdVSVjki5P268D3g48CewFNqfdNgMPpe29wCZJSyStBtYAB/rd8bpxeOfP89o2KBe1sc8yYFdaGfIaYE9EfFHSw8AeSbcBzwDvBoiII5L2AE8AZ4HbI+JcOd2vl4hAEpL8w58Zh7YN0oLBHRHfBq5r0f4ccNMcx+wAdvTcuxHk8M5L8S8k/3/ZoPjMyQrytEl+HNo2SA7uinJ4V5+nR2xYHNwV5vCuLoe2DZODu+Ic3tXj0LZhc3BnwOFdHQ5tqwIHdyYc3sPVXOkDDm0bPgd3Rhzew+fQtipwcGfG4T14rrStahzcGSqGtwO8PJ4esapycGeqGCSjGN7NUC3rl5fPiLQqa+ezSqyiZlfedQ6YhcK5n+N3lW1V5+Cugbp9vsmw/oJwlW258FRJzeQ+bZJ7/80GwRV3TTSrbujvtEGZqhTSnh6xnDi4a6TVapMqBlEVAxuq+W9l1oqnSmpoVFecdDpWh7blyhV3TdV5xcndd9993u3t27d3dLwD23LnirvmqniyTi9hOTu0m22t2ltxaFsdtPNlwRdLOiDpMUlHJG1P7dskfU/SoXS5pXDMVkmTko5JurnMAdjC6jJ10m44z8WhbXXRzlTJGeBtEfGSpMXANyT9cbrv4xHx0eLOktYCm4BrgTcAfyrpTf7C4OGaveqk2VYlvU6BzBXsDmyrmwUr7mh4Kd1cnC7zvfo3Arsj4kxEPA1MAut77qn1LCIqW33PNQXSi9nTQw5tq4u25rglLZJ0CDgN7IuIR9Jdd0j6tqT7JV2R2pYDJwqHT6W22Y+5RdKEpImZmZkehmCdKgZ4mZ/3sVAfmuYL6G7Cu1VgO7StTtoK7og4FxHrgBXAeklvBj4BXAOsA6aBe9LurRLggp+aiNgZEeMRMT42NtZV5603s8OsShV4UbvBu23bNge2jYSOVpVExPPA14ANEXEqBfrLwCd5dTpkClhZOGwFcLIPfbUSNMNt2BV4O7Zt28a2bdta3rd9+/bz5u4d2FZn7awqGZN0edp+HfB24ElJywq7vQs4nLb3ApskLZG0GlgDHOhvt20QqhjerRTfxHRg2yhoZ1XJMmCXpEU0gn5PRHxR0h9KWkdjGuQ48D6AiDgiaQ/wBHAWuN0rSvLQ6tt1ynxzr/l4c1XRc5lrtYlD20aFqvBiHx8fj4mJiWF3w1poVXWX8ZqZK7yb7XNV/1V4/ZqVYXx8nImJiZYvfAe3tW2+qZN+vY6a89iDeC6zKpsvuP1ZJda2+b6ouJugneuYVlMhDmuzVzm4rWOtQnS+4O7mTU4HtdncHNzWF/04I9NhbdYeB7f1nQPYrFz+WFczs8w4uM3MMuPgNjPLjIPbzCwzDm4zs8w4uM3MMuPgNjPLjIPbzCwzDm4zs8w4uM3MMuPgNjPLjIPbzCwzDm4zs8w4uM3MMuPgNjPLjIPbzCwzlfiyYEkzwF8Dzw67LyVYiseVm7qOzePKy09GxFirOyoR3ACSJiJifNj96DePKz91HZvHVR+eKjEzy4yD28wsM1UK7p3D7kBJPK781HVsHldNVGaO28zM2lOlitvMzNrg4DYzy8zQg1vSBknHJE1KumvY/emUpPslnZZ0uNB2paR9kp5K11cU7tuaxnpM0s3D6fXCJK2U9FVJRyUdkfT+1J712CRdLOmApMfSuLan9qzH1SRpkaRvSfpiul2XcR2X9LikQ5ImUlstxtaViBjaBVgE/AXwd4DXAo8Ba4fZpy7G8I+AnwUOF9p+F7grbd8F/E7aXpvGuARYnca+aNhjmGNcy4CfTduXAt9J/c96bICAS9L2YuAR4K25j6swvn8DfBr4Yl1ei6m/x4Gls9pqMbZuLsOuuNcDkxHx3Yj4W2A3sHHIfepIRHwd+P6s5o3ArrS9C7i10L47Is5ExNPAJI1/g8qJiOmIeDRtvwgcBZaT+dii4aV0c3G6BJmPC0DSCuCXgN8vNGc/rnnUeWzzGnZwLwdOFG5PpbbcXR0R09AIQOCq1J7leCWtAq6jUZ1mP7Y0nXAIOA3si4hajAv4r8C/B14utNVhXND45foVSQclbUltdRlbxy4a8vOrRVud1ydmN15JlwCfBz4QES9IrYbQ2LVFWyXHFhHngHWSLgcelPTmeXbPYlySfhk4HREHJd3YziEt2io3roIbIuKkpKuAfZKenGff3MbWsWFX3FPAysLtFcDJIfWln05JWgaQrk+n9qzGK2kxjdD+VER8ITXXYmwAEfE88DVgA/mP6wbgn0o6TmPK8W2SHiD/cQEQESfT9WngQRpTH7UYWzeGHdzfBNZIWi3ptcAmYO+Q+9QPe4HNaXsz8FChfZOkJZJWA2uAA0Po34LUKK3vA45GxMcKd2U9NkljqdJG0uuAtwNPkvm4ImJrRKyIiFU0fo7+T0T8OpmPC0DS6yVd2twG3gEcpgZj69qw3x0FbqGxYuEvgA8Nuz9d9P8zwDTwIxq/6W8DfgLYDzyVrq8s7P+hNNZjwC8Ou//zjOvnafx5+W3gULrckvvYgJ8BvpXGdRj4cGrPelyzxngjr64qyX5cNFadPZYuR5o5UYexdXvxKe9mZpkZ9lSJmZl1yMFtZpYZB7eZWWYc3GZmmXFwm5llxsFtZpYZB7eZWWb+P6qBojxL8MvsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The size of state is: \", env.observation_space.shape)\n",
    "print(\"No. of Actions: \", env.action_space.n)\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "env.close()\n",
    "plt.title('Original Frame')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play():\n",
    "    env.reset()\n",
    "    while True:\n",
    "        env.render(mode='rgb_array')\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            env.close()\n",
    "            break\n",
    "random_play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Creating Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size = 2, action_size=2):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 16)\n",
    "        self.fc2 = nn.Linear(16, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Creating out agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 1.0            # discount factor\n",
    "LR = 1e-2              # learning rate \n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Q-Network\n",
    "        self.policy_net = Policy(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LR)\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.st = 0\n",
    "        self.state_positions = []\n",
    "        self.total_reward = 0\n",
    "        self.rewards = []\n",
    "        \n",
    "    \n",
    "    def step(self, state_position, log_probs):\n",
    "        # Save rewards and log_probs\n",
    "        self.state_positions.append(state_position)\n",
    "        self.saved_log_probs.append(log_probs)\n",
    "        self.st += 1 \n",
    "        \n",
    "                \n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.policy_net(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.saved_log_probs = []\n",
    "        self.st = 0\n",
    "        self.state_positions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def learn(self, gamma):\n",
    "        discounts = [gamma**i for i in range(len(self.rewards)+1)]\n",
    "        R = sum([a*b for a,b in zip(discounts, self.rewards)])\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob in self.saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def epsiode_end(self):\n",
    "        max_h = max(self.state_positions)\n",
    "        sft = 300\n",
    "        total_reward = (1.2 + max_h) * sft\n",
    "        self.rewards = [(float(total_reward) / self.st) for i in range(0, self.st)]\n",
    "        self.total_reward = int(total_reward - self.st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Watching untrained agent play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=2, action_size= 2)\n",
    "\n",
    "# watch an untrained agent\n",
    "state = env.reset()\n",
    "agent.reset()\n",
    "for j in range(200):\n",
    "    action, _ = agent.act(state)\n",
    "    env.render()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Loading Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "scores = []\n",
    "scores_window = deque(maxlen=100)\n",
    "\n",
    "# To Load checkpoint uncomment code\n",
    "# checkpoint = torch.load('mountain_climb_solved.pth')\n",
    "# agent.policy_net.load_state_dict(checkpoint['state_dict'])\n",
    "# agent.target_net.load_state_dict(checkpoint['state_dict'])\n",
    "# agent.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# start_epoch = checkpoint['epoch']\n",
    "# scores = checkpoint['scores']\n",
    "\n",
    "# index = 1\n",
    "# for i in reversed(scores):\n",
    "#     scores_window.append(i)\n",
    "#     if index == 100:\n",
    "#         break\n",
    "#     index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Train the Agent with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes=2000):\n",
    "    steps = deque(maxlen=100)\n",
    "    for i_episode in range(start_epoch+1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        step = 0\n",
    "        while True:\n",
    "            action, log_probs = agent.act(state.copy())\n",
    "            state, reward, done, _ = env.step(0 if action == 0 else 2)\n",
    "            agent.step(state[0], log_probs)\n",
    "            step += 1\n",
    "            if done:\n",
    "                agent.epsiode_end()\n",
    "                break\n",
    "        agent.learn(GAMMA)\n",
    "        steps.append(step)\n",
    "        scores_window.append(agent.total_reward)       # save most recent score\n",
    "        scores.append(agent.total_reward)              # save most recent score\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(steps) < 150:\n",
    "            state = {'epoch': i_episode,'state_dict': agent.policy_net.state_dict(),'optimizer': agent.optimizer.state_dict(),\n",
    "                     'scores': scores }\n",
    "            torch.save(state, \"mountain_car_solved{}.pth\".format(i_episode))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 41.41\n",
      "Episode 200\tAverage Score: 24.75\n",
      "Episode 300\tAverage Score: 14.99\n",
      "Episode 400\tAverage Score: 10.87\n",
      "Episode 500\tAverage Score: 12.05\n",
      "Episode 600\tAverage Score: 14.57\n",
      "Episode 700\tAverage Score: 12.91\n",
      "Episode 800\tAverage Score: 10.95\n",
      "Episode 900\tAverage Score: 12.08\n",
      "Episode 1000\tAverage Score: 16.90\n",
      "Episode 1100\tAverage Score: 39.53\n",
      "Episode 1200\tAverage Score: 45.39\n",
      "Episode 1300\tAverage Score: 48.46\n",
      "Episode 1400\tAverage Score: 43.11\n",
      "Episode 1500\tAverage Score: 37.19\n",
      "Episode 1600\tAverage Score: 46.87\n",
      "Episode 1700\tAverage Score: 55.41\n",
      "Episode 1800\tAverage Score: 61.91\n",
      "Episode 1900\tAverage Score: 61.02\n",
      "Episode 2000\tAverage Score: 62.81\n",
      "Episode 2100\tAverage Score: 54.63\n",
      "Episode 2200\tAverage Score: 33.33\n",
      "Episode 2300\tAverage Score: 23.56\n",
      "Episode 2400\tAverage Score: 22.00\n",
      "Episode 2500\tAverage Score: 17.72\n",
      "Episode 2600\tAverage Score: 11.88\n",
      "Episode 2700\tAverage Score: 11.38\n",
      "Episode 2800\tAverage Score: 10.32\n",
      "Episode 2900\tAverage Score: 9.781\n",
      "Episode 3000\tAverage Score: 9.68\n",
      "Episode 3100\tAverage Score: 9.221\n",
      "Episode 3200\tAverage Score: 14.87\n",
      "Episode 3300\tAverage Score: 11.40\n",
      "Episode 3400\tAverage Score: 10.36\n",
      "Episode 3500\tAverage Score: 13.48\n",
      "Episode 3600\tAverage Score: 11.06\n",
      "Episode 3700\tAverage Score: 12.18\n",
      "Episode 3800\tAverage Score: 7.570\n",
      "Episode 3900\tAverage Score: 9.34\n",
      "Episode 3917\tAverage Score: 8.71"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d3f358340f3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# plot the scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m111\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-d22d64b79b85>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(n_episodes)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-afa6752ad9b1>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepanshu\\.conda\\envs\\ai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-60d629279583>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepanshu\\.conda\\envs\\ai\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = train(10000)\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch an trained agent\n",
    "state = env.reset()\n",
    "agent.reset()\n",
    "for j in range(200):\n",
    "    action, _ = agent.act(state)\n",
    "    env.render()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break \n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
