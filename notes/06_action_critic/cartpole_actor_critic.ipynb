{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic\n",
    "\n",
    "In this notebook, we will implement Actor Critic algorithm to play Catpole.\n",
    "\n",
    "## Step 1: Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepanshu\\.conda\\envs\\ai\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Viewing our Enviroment\n",
    "Execute the code cell below to play Cartpole with a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of state is:  (4,)\n",
      "No. of Actions:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The size of state is: \", env.observation_space.shape)\n",
    "print(\"No. of Actions: \", env.action_space.n)\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "env.close()\n",
    "plt.title('Original Frame')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play():\n",
    "    env.reset()\n",
    "    while True:\n",
    "        env.render(mode='rgb_array')\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            env.close()\n",
    "            break\n",
    "random_play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Creating Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, state_size = 4, action_size=2):\n",
    "        super(Network, self).__init__()\n",
    "        self.action_size = action_size\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.from_numpy(x).float().unsqueeze(0).to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        if self.action_size == 1:\n",
    "            return x\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Creating out agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99            # discount factor\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, alpha=0.0001, beta=0.0005):\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Actor\n",
    "        self.actor_net = Network(state_size, action_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_net.parameters(), lr=alpha)\n",
    "        \n",
    "        # Critic\n",
    "        self.critic_net = Network(state_size, 1).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_net.parameters(), lr=beta)\n",
    "        \n",
    "        self.log_probs = None\n",
    "        \n",
    "                \n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        probs = self.actor_net(state)\n",
    "        action_probs = Categorical(probs)\n",
    "        action = action_probs.sample()\n",
    "        self.log_probs = action_probs.log_prob(action)\n",
    "        return action.item()\n",
    "        \n",
    "    def step(self, state, reward, new_state, done):        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        \n",
    "        cv_new_state = self.critic_net(new_state)\n",
    "        cv_state = self.critic_net(state)\n",
    "        \n",
    "        reward = torch.tensor(reward, dtype=torch.float).to(device)\n",
    "        \n",
    "        delta = reward + GAMMA * cv_new_state * (1-int(done)) - cv_state\n",
    "        \n",
    "        actor_loss = -self.log_probs * delta\n",
    "        critic_loss = delta**2\n",
    "        \n",
    "        (actor_loss + critic_loss).backward()\n",
    "        \n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Watching untrained agent play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=4, action_size= env.action_space.n)\n",
    "\n",
    "# watch an untrained agent\n",
    "state = env.reset()\n",
    "for j in range(200):\n",
    "    action = agent.act(state)\n",
    "    env.render()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    time.sleep(0.05)\n",
    "    if done:\n",
    "        break \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Loading Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "scores = []\n",
    "scores_window = deque(maxlen=100)\n",
    "\n",
    "# To Load checkpoint uncomment code\n",
    "# checkpoint = torch.load('cartpole_ac_solved.pth')\n",
    "# agent.actor_net.load_state_dict(checkpoint['actor_dict'])\n",
    "# agent.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])\n",
    "# agent.critic_net.load_state_dict(checkpoint['critic_dict'])\n",
    "# agent.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])\n",
    "# start_epoch = checkpoint['epoch']\n",
    "# scores = checkpoint['scores']\n",
    "\n",
    "# index = 1\n",
    "# for i in reversed(scores):\n",
    "#     scores_window.append(i)\n",
    "#     if index == 100:\n",
    "#         break\n",
    "#     index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Train the Agent with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes=2000):\n",
    "    for i_episode in range(start_epoch+1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        while True:\n",
    "            action = agent.act(state.copy())\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            # reward = -100 if done and score < 450 else reward  \n",
    "            agent.step(state, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=490:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            state = {'epoch': i_episode,'actor_dict': agent.actor_net.state_dict(),'actor_optimizer': agent.actor_optimizer.state_dict(),\n",
    "                     'critic_dict': agent.critic_net.state_dict(),'critic_optimizer': agent.critic_optimizer.state_dict(), 'scores': scores }\n",
    "            torch.save(state, \"cartpole_ac_solved.pth\")\n",
    "            break\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1500\tAverage Score: 442.25\n",
      "Episode 1545\tAverage Score: 440.58"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-693372cdd11e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# plot the scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m111\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-2a6498eb6a9d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(n_episodes)\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;31m# reward = -100 if done and score < 450 else reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-015006e6be5e>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, reward, new_state, done)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mactor_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepanshu\\.conda\\envs\\ai\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = train(2000)\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch an trained agent\n",
    "state = env.reset()\n",
    "score = 0\n",
    "while True:\n",
    "    action = agent.act(state)\n",
    "    env.render()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    score += reward\n",
    "    state = next_state\n",
    "    if done:\n",
    "        print(\"Your score is: \", score)\n",
    "        break \n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
